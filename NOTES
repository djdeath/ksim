							-*- mode: outline -*-

* GEM/Stub

* Stages

** Geometry

** Tesselation

** Compute

* Fixed function

** Depth buffers

HiZ.

** Stencil

** Blending

* Thread pool

Group primitives per tile, maybe fixed size per-tile queue, then run all shaders for
one tile on one cpu-core.  Should reduce contention, allow one cpu to have entire
RT tile (page) in cache the whole time.

* JIT

** Cache programs if JITer gets too slow

** JIT in ps thread setup code

We can generate code to copy in the exact payload we'll need. Also
compile in depth test. Maybe even entire tile level loop to avoid
function pointer dispatch per SIMD8 group. This should use the
register allocator so that ideally for small shader we might never
need to write out the payload.  Compile in RT write, blending, srgb
conversion.

** Track constants per sub reg (use case: msg headers)

** Detect constant offset ubo loads

Since we JIT at 3DPRIMITIVE time, we know which buffers are bound when
we JIT, so if we see a constant offset load through sampler or constant
cache (both read only cached) we can just look up the constant and compile
it into the shader as a regular load.

* WM

** SIMD16 dispatch

** Use AVX2

** Perspective correct barycentric

** Multi-sample

** Lines, points

* EU

** Use immediate AVX2 shift when EU operand is an immediate

** All the instructions

** Indirect addressing

** Control flow

** Write masks

Can ignore outside control flow, inside control flow we can use avx2
blend to implement write masking.

** Execution size greater than 8

* Misc

** Hybrid HW passthrough mode.

Run on hw for a while, then switch to simulator when something
triggers. Switch back.

* KIR

** Fix tail call optimization for eot sends/calls. For example, this:

00a8  send(8)         g124<1>UW       g13<8,8,1>F
                            sampler ld_lz SIMD8 Surface = 1 Sampler = 0 mlen 3 rlen 4 { align1 1Q };
00b8  sendc(8)        null<1>UW       g124<8,8,1>F
                            render RT write SIMD8 LastRT Surface = 0 mlen 4 rlen 0 { align1 1Q EOT };

becomes

       const_send src g13-g15, dst g124-g127  lea    -0x10e7(%rip),%rsi        # 0x0000000000000000
                                            push   %rdi
                                            callq  0x00000000082a44a2
                                            pop    %rdi
       send src g124-g127                   lea    -0x10b5(%rip),%rsi        # 0x0000000000000040
                                            push   %rdi
                                            callq  0x00000000082a0a76
                                            pop    %rdi
       eot                                  retq   

which could just be

       const_send src g13-g15, dst g124-g127  lea    -0x10e7(%rip),%rsi        # 0x0000000000000000
                                            callq  0x00000000082a44a2
       send src g124-g127                   lea    -0x10b5(%rip),%rsi        # 0x0000000000000040
                                            jmp  0x00000000082a0a76

since we don't need rdi anymore at that point and we can instead of
call for the last call that ends the thread.

** Detect same vb strides

Only compute each vid * stride once, so that buffers with the same
stride don't generate the same computation.

** Spill registers that hold regions or immediates

When choosing a register to spill, try to find one that holds an
immediate, and avoid spilling. Make unspill just reload the value. Can
be done for regions too, but needs analysis to determine that region
is unchanged. Maybe rewrite grf access to be SSA?
