							-*- mode: outline -*-

* GEM/Stub

* Stages

** Geometry

** Tesselation

** Compute

* Fixed function

** Depth buffers

HiZ.

** Stencil

** Blending

* Thread pool

Group primitives per tile, maybe fixed size per-tile queue, then run all shaders for
one tile on one cpu-core.  Should reduce contention, allow one cpu to have entire
RT tile (page) in cache the whole time.

* JIT

** Cache programs if JITer gets too slow

** JIT in ps thread setup code

We can generate code to copy in the exact payload we'll need. Also
compile in depth test. Maybe even entire tile level loop to avoid
function pointer dispatch per SIMD8 group. This should use the
register allocator so that ideally for small shader we might never
need to write out the payload.  Compile in RT write, blending, srgb
conversion.

** Track constants per sub reg (use case: msg headers)

** Detect constant offset ubo loads

Since we JIT at 3DPRIMITIVE time, we know which buffers are bound when
we JIT, so if we see a constant offset load through sampler or constant
cache (both read only cached) we can just look up the constant and compile
it into the shader as a regular load.

* WM

** SIMD16 dispatch

** Use AVX2

** Perspective correct barycentric

** Multi-sample

** Lines, points

* EU

** Use immediate AVX2 shift when EU operand is an immediate

** All the instructions

** Indirect addressing

** Control flow

** Write masks

Can ignore outside control flow, inside control flow we can use avx2
blend to implement write masking.

** Execution size greater than 8

* Misc

** Hybrid HW passthrough mode.

Run on hw for a while, then switch to simulator when something
triggers. Switch back.

** KIR

* Optimize redundant rax loads for gather

* Fix tail call optimization for eot sends/calls
